{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XO_ksRQaGHz"
   },
   "source": [
    "Version: 2019.12.18\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76k0MaK2sz7C"
   },
   "source": [
    "# Intelligent Systems - Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd8MkLjYsz7F"
   },
   "source": [
    "# Aufgabe 1 - Information Gain\n",
    "\n",
    "* Aus der Vorlesung kennen wir die Formeln für die Restentropie und den Information Gain:\n",
    "$$R= \\sum_{i=1}^{\\nu} \\frac{|T_i|}{|T|} \\cdot H(T_i),$$ \n",
    "$$H(T_i) = \\sum_{i=1}^{C} -p_i \\cdot log_2(p_i).$$\n",
    "\n",
    "* Der Information Gain ist: \n",
    "$$IG = H(T) - R(\\{T_i\\}),$$ \n",
    "\n",
    "* Rufen Sie sich auch den Pseudocode für das Anlernen eines Decision Tree Models ins Gedächtnist:\n",
    "![DTL](https://docs.google.com/uc?id=1U6sPhZkVc0VAykERjN0bJ3ImWU-zNtge)\n",
    "\n",
    "## Aufgabe\n",
    "\n",
    "Betrachten Sie ein Datenset aus 400 Beispiele der Klasse C1 und 400 Beispiele der Klasse C2.\n",
    "Nehmen Sie an, ein Decision Tree Model A teilt diese Beispiele in (300,100) für den ersten Blattknoten und (100,300) für den zweiten Blattknoten, wobei (n,m) bedeutet, dass n Beispiele zu C1 gehören und m Beispiele zu C2.\n",
    "Genau so teilt ein Decision Tree Model B die Beispiele in (200,400) und (200,0).\n",
    "\n",
    "**(1)** Berechnen Sie die Misclassification Rate für beide DTMs und zeigen Sie, dass diese gleich sind!*\n",
    "\n",
    "**(2)** Berechnen Sie den Information Gain für beide DTMs und zeigen Sie, dass der Information Gain von DTM $B$ größer ist als von $A$!*\n",
    "\n",
    "**(3)** Vervollständigen Sie den Code zur Berechnung der Misclassification Rate und  des Information Gains!*\n",
    "\n",
    "**(4)** Gegeben seien zwei binäre Variablen $A$ und $B$, mit denen ein Decision Tree zur Klassifikation trainiert wird. Dieser verwendet die XOR-Funktion. Die Trainingsdaten bestehen aus Beispielen für alle möglichen Kombinationen von Attributen und Klassen. Anschließend werden ungelabelte (Klasse nicht bekannt) Daten mit allen möglichen Werten für $A$ und $B$ mit dem trainierten Decision Tree klassifiziert. Die klaissifizierten Daten werden wiederrum für das Training eines neuen Decision Trees genutzt. Erhalten wir den gleichen Decision Tree?*\n",
    "\n",
    "## Lösung:\n",
    "\n",
    "### Misclassification Rates\n",
    "\n",
    "DTM $A$: Von den $800$ Beispielen werden im ersten und zweiten Blattknoten insgesamt 100+100=200 falsch klassifiziert. Daher ist die Misclassification Rate $25\\%$.\n",
    "\n",
    "DTM $B$: Von den $800$ Beispielen werden im ersten und zweiten Blattknoten insgesamt 200+0=200 falsch klassifiziert. (Beachte, dass der erste Blattknoten Klasse $C_2$ vorhersagt, da es die Majorität ist.). Daher ist die Misclassification Rate $25\\%$.\n",
    "\n",
    "Beide Raten sind gleich. \n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Aus der Vorlesung kennen wir die Formeln für die Restentropie und den Information Gain:\n",
    "\n",
    "$$R= \\sum_{i=1}^{\\nu} \\frac{|T_i|}{|T|} \\cdot H(T_i),$$ \n",
    "$$H(T_i) = \\sum_{i=1}^{C} -p_i \\cdot log_2(p_i).$$\n",
    "\n",
    "Der Information Gain ist\n",
    "$$IG = H(T) - R(\\{T_i\\}),$$ \n",
    "wobei $T$ in $T_i$ aufgeteilt wird.\n",
    "\n",
    "\n",
    "DTM $A$:\n",
    "\n",
    "Es ist $H(T) = 1$, da die Beispiele gleichverteilt sind.\n",
    "\n",
    "Der erste Blattknoten hält den Anteil $0.5$ der Beispiele mit Wahrscheinlichkeiten $(0.75, 0.25)$.\n",
    "\n",
    "Der zweite Blattknoten hält den Anteil $0.5$ der Beispiele mit Wahrscheinlichkeiten $(0.25, 0.75)$.\n",
    "\n",
    "$$R= -2.0 \\cdot 0.5 \\cdot \\left(0.25 \\cdot log_2(0.25)+0.75 \\cdot log_2(0.75)\\right) $$\n",
    "\n",
    "DTM $B$: \n",
    "\n",
    "Es ist $H(T) = 1$, da die Beispiele gleichverteilt sind.\n",
    "\n",
    "Der erste Blattknoten hält den Anteil $0.75$ der Beispiele mit Wahrscheinlichkeiten $(\\frac{1}{3}, \\frac{2}{3})$.\n",
    "\n",
    "Der zweite Blattknoten hält den Anteil $0.25$ der Beispiele mit Wahrscheinlichkeiten $(1.00, 0.00)$.\n",
    "\n",
    "$$R= -0.75 \\cdot \\left(\\frac{1}{3} \\cdot log_2(\\frac{1}{3})+\\frac{2}{3} \\cdot log_2(\\frac{2}{3})\\right) + 0.0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGKoPxpQsz7I"
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "ra = -0.25*log2(0.25) + -0.75*log2(0.75)\n",
    "iga = 1 - ra\n",
    "\n",
    "print(\"Remaining Entropy R for tree A: \", ra)\n",
    "print(\"Information Gain for tree A: \", iga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewszVKvnsz7S"
   },
   "outputs": [],
   "source": [
    "rb = 0.75 * ( -1/3*log2(1/3) + -2/3*log2(2/3) )\n",
    "igb = 1 - rb\n",
    "print(\"Remaining Entropy R for tree B\", rb)\n",
    "print(\"Information Gain for tree B: \", igb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVBQctXjyfiy"
   },
   "source": [
    "**(4)** \n",
    "\n",
    "Training Data:\n",
    "\n",
    "A | B | C \n",
    "--|---|---\n",
    "1 | 1 | 0 \n",
    "1 | 0 | 1\n",
    "0 | 1 | 1 \n",
    "0 | 0 | 0\n",
    "\n",
    "Kein Informationsgewinn beim ersten Attribut, egal welches wir wählen: der Zufall (Implementierung) entscheidet:\n",
    "\n",
    "```\n",
    "A == 0?\n",
    "\n",
    "ja |                                            \\ nein\n",
    "\n",
    "B == 0?                                         B == 0?\n",
    "ja |               \\ nein                    ja |               \\ nein\n",
    "\n",
    "C=0               C=1                        C=1              C=0 \n",
    "```\n",
    "\n",
    "**Zweiter Teil der Frage:**\n",
    "Alle Attributkombinationen sind schon im ersten Teil gegeben - die Klassenzuordnung wird wie im ersten Teil sein (XOR Funktion). Der Baum kann aber jetzt anders strukturiert sein $\\rightarrow$ Attribut B kann zuerst gewählt werden. An der Funktion des Baumes ändert das aber nichts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG76tMZgsz7Z"
   },
   "source": [
    "# Aufgabe 2 - Recursive Split Algorithm\n",
    "\n",
    "![DTL](https://docs.google.com/uc?id=1U6sPhZkVc0VAykERjN0bJ3ImWU-zNtge)\n",
    "\n",
    "Auf der Basis der Daten der amerikanischen Volkszählung wollen wir die folgenden zwei Klassen bestimmen.\n",
    "\n",
    "    >50K$, <=50K$\n",
    "\n",
    " **Die Attribute sind**: \n",
    "\n",
    "* **age**: continuous.  \n",
    "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n",
    "* **fnlwgt**: continuous. (= final weight)\n",
    "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "* **education-num**:  continuous. \n",
    "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n",
    "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n",
    "* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. \n",
    "(Comment: this term is (was?) quite common in the american litrature, although ...  search the net for the debate in the U.S. and elsewhere.) \n",
    "* **sex**: Female, Male.  \n",
    "* **capital-gain**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **hours-per-week**: continuous.\n",
    "* **native-country**:  United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "Wir benutzen keine kontinuierlichen Attribute. \n",
    "\n",
    "Daten finden Sie auf der Website (adult.txt ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlyq1-Vtsz7c"
   },
   "source": [
    "## Aufgabe: Vervollständigen Sie den unteren Code\n",
    " **Es gibt Blanks in den folgenden Funktionen**: \n",
    "- calc_entropy()\n",
    "- calc_ig()\n",
    "- majority()\n",
    "- choose_best_attr()\n",
    "- dtree_learning()\n",
    "- dtree_classify()\n",
    "- dtree_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm1GxQa0sz7f"
   },
   "outputs": [],
   "source": [
    "\"\"\" Intelligent Systems TUD 2018, Ex.1\n",
    "\n",
    "Decision Tree Learning:\n",
    "Based on american census data you want to predict two classes of income of people:\n",
    ">50K$, <=50K$.\n",
    "\n",
    "We do not use continuous attributes for this first decision tree task.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "__author__ = 'Benjamin Guthier'\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "def openfile(path, fname):\n",
    "    \"\"\"opens the file at path+fname and returns a list of examples and attribute values.\n",
    "    examples are returned as a list with one entry per example. Each entry then\n",
    "    is a list of attribute values, one of them being the class label. The returned list attr\n",
    "    contains one entry per attribute. Each entry is a list of possible values or an empty list\n",
    "    for numeric attributes.\n",
    "    \"\"\"\n",
    "    datafile = open(path + fname, \"r\")\n",
    "    examples = []\n",
    "    for line in datafile:\n",
    "        line = line.strip()\n",
    "        line = line.strip('.')\n",
    "        # ignore empty lines. comments are marked with a |\n",
    "        if len(line) == 0 or line[0] == '|':\n",
    "            continue\n",
    "        ex = [x.strip() for x in line.split(\",\")]\n",
    "        examples.append(ex)\n",
    "\n",
    "    attr = []\n",
    "    for i in range(len(examples[0])):\n",
    "        values = list({x[i] for x in examples})  # set of all different attribute values\n",
    "        if values[0].isdigit():  # if the first value is a digit, assume all are numeric\n",
    "            attr.append([])\n",
    "        else:\n",
    "            attr.append(values)\n",
    "\n",
    "    return examples, attr\n",
    "\n",
    "\n",
    "def calc_entropy(examples, cls_index):\n",
    "    \"\"\"calculates the entropy over all examples. The index of the class label in the example\n",
    "    is given by cls_index. Can also be the index to an attribute.\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    # get attributes of examples with index cls_index\n",
    "    example_classifications = [example[cls_index] for example in examples]\n",
    "    # get unique counts of example_attributes\n",
    "    unique, counts = np.unique(example_classifications, return_counts=True)\n",
    "\n",
    "    # normalize counts to total number of examples for getting probs\n",
    "    probs = counts/len(examples)\n",
    "    # print(probs)\n",
    "\n",
    "    entropy = -np.sum(probs*np.log2(probs))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calc_ig(examples, attr_index, cls_index):\n",
    "    \"\"\"Calculates the information gain over all examples for a specific attribute. The\n",
    "    class index must be specified.\n",
    "\n",
    "    uses calc_entropy\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    # get attributes of examples with index attr_indx\n",
    "    example_attributes = [example[attr_index] for example in examples]\n",
    "\n",
    "    # get unique counts of example_attributes\n",
    "    unique, counts = np.unique(example_attributes, return_counts=True)\n",
    "\n",
    "    # example split\n",
    "    remainder = 0\n",
    "    for j in range(len(unique)):\n",
    "        # get all examples with attribute unique[j]\n",
    "        examples_unique = [example for example in examples if example[attr_index] == unique[j]]\n",
    "        remainder += len(examples_unique)/len(examples)*calc_entropy(examples_unique, cls_index)\n",
    "    return calc_entropy(examples, cls_index) - remainder\n",
    "\n",
    "\n",
    "def majority(examples, attr_index):\n",
    "    \"\"\"Returns the value of attribute \"attr_index\" that occurs the most often in the examples.\"\"\"\n",
    "    # create a flat list of all attribute values (with duplicates, so we can count)\n",
    "    attr_vals = [example[attr_index] for example in examples]\n",
    "\n",
    "    # among all unique attribute values, find the maximum with regards to occurrence in the attr_vals list\n",
    "\n",
    "    maj_attr_val = max(set(attr_vals), key=attr_vals.count)\n",
    "    return maj_attr_val\n",
    "\n",
    "def choose_best_attr(examples, attr_avail, cls_index):\n",
    "    \"\"\"Iterates over all available attributes, calculates their information gain and returns the one\n",
    "    that achieves the highest. attr_avail is a list of booleans corresponding to the list of attributes.\n",
    "    it is true if the attribute has not been used in the tree yet (and is not numeric).\n",
    "    \"\"\"\n",
    "    igs = []  # list of information gains for each attribute\n",
    "\n",
    "    for j in range(len(attr)):\n",
    "        if attr_avail[j]:\n",
    "            igs.append(calc_ig(examples, j, cls_index))\n",
    "        else:\n",
    "            igs.append(-1)\n",
    "\n",
    "    return igs.index(max(igs))  # return index of the attribute with highest IG\n",
    "\n",
    "\n",
    "def dtree_learning(examples, attr_avail, default, cls_index):\n",
    "    \"\"\"Implementation of the decition tree learning algorithm according to the pseudo code\n",
    "    in the lecture. Receives the remaining examples, the remaining attributes (as boolean list),\n",
    "    the default label and the index of the class label in the attribute vector.\n",
    "    Returns the root node of the decision tree. \n",
    "\n",
    "    Each tree node is a tuple where the first entry is the index of the \n",
    "    attribute that has been used for the split. It is \"None\" for leaf nodes.\n",
    "    The second entry is a list of subtrees of the same format. The subtrees are ordered in the\n",
    "    same way as the attribute values in \"attr\". For leaf nodes, the second entry is the predicted class.\n",
    "\n",
    "    uses choose_best_attr, majority, dtree_learning\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    if len(examples) == 0:\n",
    "        # is leaf\n",
    "        # no examples left -> majority class prediction of parent\n",
    "        return [None, default]\n",
    "    elif len({x[cls_index] for x in examples}) == 1:\n",
    "        # is leaf\n",
    "        # uniform classes\n",
    "        return [None, examples[0][cls_index]]\n",
    "    elif attr_avail.count(True) == 0:\n",
    "        # is leaf\n",
    "        # no attribute left to split -> majority class prediction\n",
    "        return [None, majority(examples, cls_index)]\n",
    "    else:\n",
    "        best = choose_best_attr(examples, attr_avail, cls_index)\n",
    "        tree = [best, []]\n",
    "        for v in attr[best]:\n",
    "            examples_v = [x for x in examples if x[best] == v]\n",
    "            # why is it important to make a copy and not change it directly?\n",
    "            new_attr_avail = attr_avail.copy()\n",
    "            new_attr_avail[best] = False\n",
    "            subtree = dtree_learning(examples_v, new_attr_avail, majority(examples, cls_index), cls_index)\n",
    "            tree[1].append(subtree)\n",
    "        return tree\n",
    "\n",
    "\n",
    "def dtree_classify(dtree, x):\n",
    "    \"\"\"Classifies a single example x using the given decision tree. Returns the predicted class label.\n",
    "    \"\"\"\n",
    "    # attribute index of splitting attribute\n",
    "    attr_split_index = dtree[0]\n",
    "\n",
    "    if attr_split_index is not None:\n",
    "        # get attribute value for example\n",
    "        example_split_attr = x[attr_split_index]\n",
    "\n",
    "        # subtree position\n",
    "        subtree_pos = attr[attr_split_index].index(example_split_attr)\n",
    "\n",
    "        return dtree_classify(dtree[1][subtree_pos], x)  # descend into subtree recursively\n",
    "    else:\n",
    "        return dtree[1]\n",
    "\n",
    "\n",
    "def dtree_test(dtree, examples, cls_index):\n",
    "    \"\"\"Classify all examples using the given decision tree. Prints the achieved accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    for j in range(len(examples)):\n",
    "        if dtree_classify(dtree, examples[j]) == examples[j][cls_index]:\n",
    "            correct += 1\n",
    "\n",
    "    print(\"{} out of {} correct ({:.2f}%)\".format(correct, len(examples), correct / len(examples) * 100))\n",
    "\n",
    "\n",
    "path = \"./\"  # directory of your data\n",
    "datafile = \"adult.data.txt\"\n",
    "testfile = \"adult.test.txt\"\n",
    "examples, attr = openfile(path, datafile)  # load the training set\n",
    "test, test_attr = openfile(path, testfile)  # load the test set\n",
    "cls_index = len(attr) - 1  # the last attribute is assumed to be the class label\n",
    "# attr_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "\n",
    "attr_avail = []  # marks which attributes are available for splitting (not numeric and not the class label)\n",
    "for i in range(len(attr)):\n",
    "    # the list attr[i] contains all possible values of attribute i. It is empty for numeric attributes.\n",
    "    attr_avail.append(len(attr[i]) > 0 and i != cls_index)\n",
    "\n",
    "# print(attr_avail)\n",
    "dtree = dtree_learning(examples, attr_avail, [], cls_index)\n",
    "\n",
    "print(\"Before removal of unknowns: \")\n",
    "print(\"Training Set\")\n",
    "dtree_test(dtree, examples, cls_index)\n",
    "print(\"Test Set\")\n",
    "dtree_test(dtree, test, cls_index)\n",
    "\n",
    "# Extra task removal of unknowns\n",
    "examples_removed = []\n",
    "test_removed = []\n",
    "\n",
    "for example in examples:\n",
    "    # print(example.__contains__(\"?\"))\n",
    "    if not example.__contains__(\"?\"):\n",
    "        examples_removed.append(example)\n",
    "\n",
    "for example in test:\n",
    "    if not example.__contains__(\"?\"):\n",
    "        test_removed.append(example)\n",
    "\n",
    "# print(len(examples_removed))\n",
    "# print(len(test_removed))\n",
    "dtree = dtree_learning(examples_removed, attr_avail, [], cls_index)\n",
    "\n",
    "print(\"After removal of unknowns: \")\n",
    "print(\"Training Set\")\n",
    "dtree_test(dtree, examples_removed, cls_index)\n",
    "print(\"Test Set\")\n",
    "dtree_test(dtree, test_removed, cls_index)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "08_decision_trees_solution.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
