{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhcteGUDImU2"
   },
   "source": [
    "**Änderung am 29.11.2018**\n",
    "*   Fehler in Lösung zu Aufgabe 1 behoben (Übergangswahrscheinlichkeiten von A $\\rightarrow$ Pe und Pe $\\rightarrow$ A vertauscht)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSEXtmomcSEc"
   },
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "Dieses Lab soll mit Hidden Markov Models und dem Viterbi-Algorithmus vertraut machen. Markov-Modelle sind probabilistische Modelle, die Zustandsübergänge $X_i \\rightarrow X_{j}$ modellieren. $X_i$ sind dabei Zufallsvariablen. Das besondere an Markov-Modellen ist, dass der Übergang $X_i \\rightarrow X_j$ nur vom aktuellen Zustand und nicht von der Historie der Markov-Kette abhängt. Damit lässt sich das Modell eindeutig durch die Übergangswahrscheinlichkeiten $p(X_j|X_i)$ beschreiben und als Graph darstellen. **Hidden** Markov Models sind Markov-Modelle, bei denen einige Zustände nicht beobachtet werden. \n",
    "Im Folgenden wollen wir Entitätenerkennung mit Hidden-Markov-Models durchführen. Wir versuchen Wörtern eine Kategorie zuzuordnen, bspw. Personennamen oder Städte. Dabei sind die Zustände oder *states* diese Kategorien und *observables* die Wörter. In der Modellierung werden Übergänge zwischen den Zuständen und Emissionen von Zuständen zu Wörtern benutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2q4pv1xe1xL-"
   },
   "source": [
    "## Aufgabe 1\n",
    "In dieser Aufgabe sollen Sie nun die entsprechenden Wahrscheinlichkeiten aus Trainingsdaten ablesen. Die Entitäten/Kategorien sind *people (Pe)*, *cities (St)*, *starting state $ (S)*, *final state € (E)* sowie alles andere *(A)*. Gegeben seien die folgenden Trainingssätze:\n",
    "\n",
    "-  &#36; Paris(*St*) is larger than Washington(*St*) €\n",
    "-  &#36; Hilton hotels are expensive €\n",
    "-  &#36; Paris(*Pe*) Hilton(*Pe*) went to Washington(*St*) €\n",
    "-  &#36; Denzel(*Pe*) Washington(*Pe*) stayed at the Hilton €\n",
    "-  &#36; P.(*Pe*) Hilton(*Pe*) met D.(*Pe*) Washington(*Pe*) €\n",
    "\n",
    "Zeichnen Sie das HMM-Modell mit Knoten *S*, *E*, *Pe*, *St* und *A*. Beschriften Sie die Kanten mit den entsprechenden Übergangswahrscheinlichkeiten. Spezifizieren Sie auch die Emissionswahrscheinlichkeiten für *Pe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLPrsKnBQCjF"
   },
   "source": [
    "## Lösung zu Aufgabe 1\n",
    "Übergangswahrscheinlichkeiten:\n",
    "- $P(S \\rightarrow St) = \\frac{1}{5}$\n",
    "- $P(S \\rightarrow Pe) = \\frac{3}{5}$\n",
    "- $P(S \\rightarrow A) = \\frac{1}{5}$\n",
    "- $P(St \\rightarrow A) = \\frac{1}{3}$\n",
    "- $P(St \\rightarrow E) = \\frac{2}{3}$\n",
    "- $P(A \\rightarrow St) = \\frac{2}{14}$\n",
    "- $P(A \\rightarrow A) = \\frac{9}{14}$\n",
    "- $P(A \\rightarrow Pe) = \\frac{1}{14}$\n",
    "- $P(A \\rightarrow E) = \\frac{2}{14}$\n",
    "- $P(Pe \\rightarrow Pe) = \\frac{4}{8}$\n",
    "- $P(Pe \\rightarrow E) = \\frac{1}{8}$\n",
    "- $P(Pe \\rightarrow A) = \\frac{3}{8}$\n",
    "\n",
    "Emissionswahrscheinlichkeiten für *Pe*:\n",
    "- $P(Pe \\rightarrow \\text{\"Paris\"}) = \\frac{1}{8}$\n",
    "- $P(Pe \\rightarrow \\text{\"Hilton\"}) = \\frac{2}{8}$\n",
    "- $P(Pe \\rightarrow \\text{\"Denzel\"}) = \\frac{1}{8}$\n",
    "- $P(Pe \\rightarrow \\text{\"Washington\"}) = \\frac{2}{8}$\n",
    "- $P(Pe \\rightarrow \\text{\"P.\"}) = \\frac{1}{8}$\n",
    "- $P(Pe  \\rightarrow \\text{\"D.\"}) = \\frac{1}{8}$\n",
    "\n",
    "![alt text](https://docs.google.com/uc?id=109uKbtbNS-BJMzbh2tHhwjkeayHFhGXM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBF45AXr0z5j"
   },
   "source": [
    "## Ineffizienter Ansatz\n",
    "\n",
    "In der Vorlesung wurden *states* mit $s$ und *observables* mit $o$ bezeichnet. Ziel ist es einer Observation-Kette $o_0, o_1, ... o_n$ die wahrscheinlichste Zustandskette $s_0, s_1, ..., s_n$ zuzuordnen. Aus der Vorlesung ist bekannt, dass man die Wahrscheinlichkeit für eine Zuordnung $\\textbf{o} \\rightarrow \\textbf{s}$ folgendermaßen berechnen kann:\n",
    "$$P(\\textbf{s}, \\textbf{o}) \\propto P(s_0)P(o_0|s_0)\\prod_{j=1}^n P(s_j|s_{j-1})P(o_j|s_j)$$\n",
    "Man muss also für jede mögliche Zustandskette $\\textbf{s}$ die Wahrscheinlichkeit ausrechnen, um das Maximum zu finden:\n",
    "$$\\textbf{s} = \\underset{\\textbf{s}}{\\arg\\max} \\,P(\\textbf{s}, \\textbf{o}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1v-SWfP1Apq"
   },
   "source": [
    "## Effiziente Lösung - Viterbi\n",
    "Da dies recht aufwendig ist, wird der *Viterbi-Algorithmus* eingeführt. Bezeichnen wir mit $\\Pi (j)$ die Wahrscheinlichkeiten für das Auftreten des Zustandes $j$, mit $A(i, j)$ die Übergangswahrscheinlichkeiten von Zustand $i$ nach Zustand $j$ und mit $B(i, j)$ die Emissionswahrscheinlichkeit für Zustand $i$ zu Observable $j$. Der Viterbi-Algorithmus baut dann eine Matrix $F(j, i)$ nach folgenden Formeln auf:\n",
    "\n",
    "$$F(j, 0) = \\Pi(j)\\cdot B(j, 0)$$\n",
    "$$F(j, i) = \\max_r (F(r, i-1)\\cdot A(r, j))\\cdot B(j, i)$$\n",
    "\n",
    "Die wahrscheinlichste Zustandszuordnung $\\textbf{s}$ zu einer Observation $\\textbf{o}$ ist dann gegeben durch:\n",
    "$$s_i = \\underset{s}{\\arg \\max} F(s, o_i),$$ was dem Maximum in jeder Spalte entspricht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkQrszPmF7fG"
   },
   "source": [
    "## Aufgabe 2\n",
    "Gegeben sei ein HMM mit den gleichen Zuständen wie in Aufgabe 1. Die Übergangswahrscheinlichkeiten sind gegeben mit: \n",
    "- $P(S \\rightarrow Pe) = \\frac{2}{5} = 0.4$\n",
    "- $P(Pe \\rightarrow Pe) = \\frac{1}{3} = 0.3$\n",
    "- $P(Pe \\rightarrow A) = \\frac{1}{2} = 0.5$\n",
    "- $P(A \\rightarrow Pe) = \\frac{1}{10} = 0.1$\n",
    "- $P(A \\rightarrow A) = \\frac{1}{2} = 0.5$\n",
    "- $P(A \\rightarrow St) = \\frac{2}{5} = 0.4$\n",
    "- $P(St \\rightarrow A) = \\frac{2}{5} = 0.4$\n",
    "- $P(S \\rightarrow St) = \\frac{3}{5} = 0.6$\n",
    "- $P(Pe \\rightarrow E) = \\frac{1}{5} = 0.2$\n",
    "- $P(St \\rightarrow E) = \\frac{3}{5} = 0.6$\n",
    "\n",
    "Ferner sind die Emissionswahrscheinlichkeiten angegeben mit:\n",
    "- $P(S \\rightarrow \\text{\"\\$\"}) = P(E \\rightarrow \\text{\"€\"}) = 1$\n",
    "- $P(Pe \\rightarrow \\text{\"John\"}) = 0.5$, $P(Pe \\rightarrow \\text{\"Denver\"}) = 0.25$, $P(Pe \\rightarrow \\text{\"Paris\"}) = 0.25$\n",
    "- $P(St \\rightarrow \\text{\"Washington\"}) = 0.4$, $P(St \\rightarrow \\text{\"Denver\"}) = 0.3$, $P(St \\rightarrow \\text{\"Paris\"}) = 0.2$, $P(St \\rightarrow \\text{\"Dresden\"}) = 0.1$\n",
    "- $P(A \\rightarrow \\text{\"went\"}) = 0.3$, $P(A \\rightarrow \\text{\"to\"}) = 0.3$, $P(A \\rightarrow \\text{\"lives\"}) = 0.2$, $P(A \\rightarrow \\text{\"in\"}) = 0.2$\n",
    "\n",
    "**Berechnen Sie die wahrscheinlichste Zustandsfolge für den Satz \"Paris lives in Denver\" mithilfe des Viterbi-Algorithmus per Hand!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k916M1kPSM-i"
   },
   "source": [
    "## Lösung zu Aufgabe 2\n",
    "![alt text](https://docs.google.com/uc?id=1ahpzh8YOjTL7Fjrp3SmXxNQN2Xzl8i3Q)\n",
    "\n",
    "\n",
    "Die wahrscheinlichste Zustandszuordnung ist\n",
    "-  \"\\$\" $\\rightarrow$ S\n",
    "- \"Paris\" $\\rightarrow$ Pe\n",
    "- \"lives\" $\\rightarrow$ A\n",
    "- \"in\"$\\rightarrow$ A\n",
    "- \"Denver\" $\\rightarrow$ St\n",
    "- \"€\" $\\rightarrow$ E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXY1PjSdIUIl"
   },
   "source": [
    "## Implementierung des Viterbi-Algorithmus\n",
    "Im Folgenden soll der Viterbi-Algorithmus und die Entitätenerkennung programmatisch umgesetzt werden. Dazu benötigen wir Trainingsdaten und Datenstrukturen wie im Folgenden beschrieben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hB-3vGY-i3If"
   },
   "source": [
    "## Upload der Dateien für Google-Collaboratory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQ1-SQlzivoa"
   },
   "source": [
    "Um die Trainingsdaten für das Hidden Markov Model verfügbar zu machen, gehen Sie bitte wie folgt vor:\n",
    "\n",
    "\n",
    "1.   Speichern Sie die Datei *hmm_denver.txt* auf Ihrem Gerät.\n",
    "2.   Öffnen Sie die Navigation des Notebooks und laden Sie unter *Files -> Upload* die Datei in Ihre Runtime hoch.\n",
    "3.   Anschließend kann die Datei wie gewohnt aus dem Python-Code heraus benutzt werden.\n",
    "\n",
    "Auf der [Website zur Vorlesung](http://www.biotec.tu-dresden.de/research/schroeder/teaching/intelligente-systeme.html) (BIOTEC, Schroeder, Teaching, Intelligente Systeme) finden Sie die vorbereitete Datei zum Download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_KmnCigjGi4"
   },
   "source": [
    "## Installieren benötigter Module für die Darstellung von Tabellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JwccQ1YjOtv"
   },
   "source": [
    "Zur Darstellung von Tabellen wird das Modul *tabulate* benötigt und in der folgenden Zelle in der Runtime installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "q0pmNLuzI-3R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (0.8.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3qbtgvReE_r"
   },
   "source": [
    "## Datenstruktur für HMM-Wahrscheinlichkeiten\n",
    "\n",
    "Diese Zelle definiert die Klasse \"prob\", welche die Emissions- und Übergangswahrscheinlichkeiten für das HMM-Model halten soll. Die Klasse *prob* enthält die Attribute *c*, *n* und *p*. Die Attribute *c* und *p* sind Dictionaries mit Keys der Form (state, state) oder (state, observation). *n* hat Keys der Form state. *c* zählt dabei die Übergänge der Form *state -> state* bzw. *state -> observation* in den Trainingsdaten. *n* zählt die Gesamtanzahl der Vorkommnisse von states in den Trainingsdaten. *p* gibt die Übergangs- bzw. Emissionswahrscheinlichkeiten für state -> state bzw. state -> observation an.\n",
    "Die Funktion *inc* wird aufgerufen, um einen Übergang in die Datenstruktur aufzunehmen. Die Funktion *prob* wird aufgerufen, um die Emissions- und Übergangswahrscheinlichkeiten in Form eines Dictionaries zurückzugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2p845zVJWlg"
   },
   "outputs": [],
   "source": [
    "class prob():\n",
    "    \"\"\"Counts occurances for a pair of keys (either state to state or state to observation)\n",
    "    as well as frequency of first state overall. Output are probability to transition.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.c={}\n",
    "        self.n={}\n",
    "        self.p={}\n",
    "        \n",
    "    def inc(self,k1,k2):\n",
    "        self.c[(k1,k2)] = self.c.get((k1,k2),0) + 1\n",
    "        self.n[k1] = self.n.get(k1,0) + 1\n",
    "\n",
    "    def prob(self):\n",
    "        for (k1,k2) in self.c:\n",
    "            self.p[(k1,k2)] = float(self.c[(k1,k2)]) / float(self.n[k1])\n",
    "        return self.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-9vhL6afj00"
   },
   "source": [
    "## Viterbi-Algorithmus\n",
    "\n",
    "Die nächsten Zellen definieren zum einen eine Helferfunktion, die aus einer Liste ihr Maximum und den Index des Maximums in der Liste zurückgibt. Zum anderen wird der Viterbi-Algorithmus definiert. Dieser Algorithmus benutzt den Ansatz der dynamischen Programmierung. \n",
    "Der Algorithmus ist nicht ganz vollständig. \n",
    "\n",
    "## Aufgabe 2 (cont.)\n",
    "** Vervollständigen Sie in der Funktion *viterbiAlg* die __!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKqkpAE6JdWc"
   },
   "outputs": [],
   "source": [
    "def max_pos(l):\n",
    "    \"\"\"Returns the maximum and its position.\"\"\"\n",
    "    m = max(l)\n",
    "    return(l.index(m),m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zy_jferMJgjv"
   },
   "outputs": [],
   "source": [
    "def viterbi_alg(str, transition, emission):\n",
    "    viterbi={} # dynamic programming matrix\n",
    "    pos={}     # matrix to record path for backtracking\n",
    "    obs = str.split()\n",
    "    # Zusatzaufgabe 1, just deleting all sentence delimiter\n",
    "    obs = [o.replace(\".\", \"\") for o in obs]\n",
    "    states = list(set([s for (s,o) in emission]))\n",
    "    # Init\n",
    "    for s in states:\n",
    "        viterbi[(s,0)]= float(s==\"S\")\n",
    "    # Fill matrix\n",
    "    for i in range(1,len(obs)):\n",
    "        for j in states:\n",
    "            # Fji = max F(r,i-1)*A(r,j)*B(j,i)\n",
    "            (pos[(j,i)],viterbi[(j,i)]) = max_pos([viterbi[(r,i-1)]*transition.get((r,j),0.0)*emission.get((j,obs[i]),0.0) for r in states])\n",
    "    # Output table\n",
    "    table = [[\"\"]+ obs]\n",
    "    for s in states:\n",
    "        row = [s]\n",
    "        for i in range(len(obs)):\n",
    "            row.append(viterbi[(s,i)])\n",
    "        table.append(row)\n",
    "    s = \"E\"\n",
    "    seq= [\"E\"]\n",
    "    for i in range(len(obs)-1,0,-1):\n",
    "        s = states[pos[(s,i)]]\n",
    "        seq.insert(0,s)\n",
    "    table.append([\"\"]+seq)\n",
    "    print()\n",
    "    print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMjFtPQfesnC"
   },
   "source": [
    "## Training auf dem Modell\n",
    "\n",
    "Diese Funktion nimmt als Eingabe den Pfad zur Datei *hmm_denver.txt* und gibt die Übergangs- und Emissionswahrscheinlichkeiten als Dictionaries zurück. Der Pfad ist einfach \"./hmm_denver.txt\". Für das Training werden die Folgenden Sätze verwendet. Dabei sind die states S, E, P, L und O. Die korrekte Zuweisung der observables zu den states muss in den Trainingsdaten in diesem Format angegeben werden.\n",
    "\n",
    "\n",
    "1.   SPPOOOOLE  &#36; John Denver will be playing in Denver &#36;\n",
    "2.  SLOOOOLE &#36; Denver is a city in Colorado &#36;\n",
    "3.  SPOPOOLE &#36; John and Paris went to Washington &#36;\n",
    "4.  SOOOOPPE &#36; The Oscar goes to Denzel Washington &#36;\n",
    "5.  SPOOLE &#36; Oscar lives in Washington &#36;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiMYlEFBJa5Q"
   },
   "outputs": [],
   "source": [
    "def readTraining(fn):\n",
    "    \"\"\"Data format: First word rpresents state sequence. Remaining words are observations.\n",
    "   For each character in the first word, there must be a word following.\n",
    "    Start state is S, End state is E\n",
    "    \"\"\"\n",
    "    t=prob()\n",
    "    e=prob()\n",
    "    print()\n",
    "    print(\"HMM is trained on the following data:\")\n",
    "    for line in open(fn):\n",
    "        print(line.strip())\n",
    "        l = line.split()\n",
    "        seq=list(l[0])\n",
    "        obs=l[1:]\n",
    "        if len(seq)!=len(obs):\n",
    "            print(\"Format error in line %s\"%(line))\n",
    "        else:\n",
    "            # Count transitions\n",
    "            for i in range(len(seq)-1):\n",
    "                t.inc(seq[i],seq[i+1])\n",
    "            # Count emissions\n",
    "            for i in range(len(seq)):\n",
    "                e.inc(seq[i],obs[i])\n",
    "    return (t.prob(), e.prob())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7XaUgSYgUyo"
   },
   "source": [
    "Die nächste Zelle liest die Trainingsdaten ein und trainiert das HMM-Modell auf diesen Daten. Ferner werden die Übergangs- und Emissionswahrscheinlichkeiten ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBMuZjUsJmKn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM is trained on the following data:\n",
      "SPPOOOOLE $ John Denver will be playing in Denver $\n",
      "l is:\n",
      "['SPPOOOOLE', '$', 'John', 'Denver', 'will', 'be', 'playing', 'in', 'Denver', '$']\n",
      "SLOOOOLE $ Denver is a city in Colorado $\n",
      "l is:\n",
      "['SLOOOOLE', '$', 'Denver', 'is', 'a', 'city', 'in', 'Colorado', '$']\n",
      "SPOPOOLE $ John and Paris went to Washington $\n",
      "l is:\n",
      "['SPOPOOLE', '$', 'John', 'and', 'Paris', 'went', 'to', 'Washington', '$']\n",
      "SOOOOPPE $ The Oscar goes to Denzel Washington $\n",
      "l is:\n",
      "['SOOOOPPE', '$', 'The', 'Oscar', 'goes', 'to', 'Denzel', 'Washington', '$']\n",
      "SPOOLE $ Oscar lives in Washington $\n",
      "l is:\n",
      "['SPOOLE', '$', 'Oscar', 'lives', 'in', 'Washington', '$']\n",
      "\n",
      "Transitions:\n",
      "L -> E: 0.80\n",
      "O -> O: 0.65\n",
      "S -> P: 0.60\n",
      "P -> O: 0.57\n",
      "P -> P: 0.29\n",
      "O -> L: 0.24\n",
      "S -> L: 0.20\n",
      "L -> O: 0.20\n",
      "S -> O: 0.20\n",
      "P -> E: 0.14\n",
      "O -> P: 0.12\n",
      "\n",
      "Emissions:\n",
      "S -> $: 1.00\n",
      "E -> $: 1.00\n",
      "L -> Denver: 0.40\n",
      "L -> Washington: 0.40\n",
      "P -> John: 0.29\n",
      "L -> Colorado: 0.20\n",
      "O -> in: 0.18\n",
      "P -> Denver: 0.14\n",
      "P -> Paris: 0.14\n",
      "P -> Denzel: 0.14\n",
      "P -> Washington: 0.14\n",
      "P -> Oscar: 0.14\n",
      "O -> to: 0.12\n",
      "O -> will: 0.06\n",
      "O -> be: 0.06\n",
      "O -> playing: 0.06\n",
      "O -> is: 0.06\n",
      "O -> a: 0.06\n",
      "O -> city: 0.06\n",
      "O -> and: 0.06\n",
      "O -> went: 0.06\n",
      "O -> The: 0.06\n",
      "O -> Oscar: 0.06\n",
      "O -> goes: 0.06\n",
      "O -> lives: 0.06\n"
     ]
    }
   ],
   "source": [
    "transition={}\n",
    "emission={}\n",
    "(transition, emission) = readTraining(\"hmm_denver.txt\")\n",
    "observations = set([o for (s,o) in emission])\n",
    "\n",
    "print()\n",
    "print(\"Transitions:\")\n",
    "t = sorted(transition.items(), key=lambda x: x[1], reverse=True)\n",
    "for ((s1,s2),v) in t:\n",
    "    print(\"%s -> %s: %0.2f\"%(s1,s2,v))\n",
    "\n",
    "print()\n",
    "print(\"Emissions:\")\n",
    "e = sorted(emission.items(), key=lambda x: x[1], reverse=True)\n",
    "for ((s,o),v) in e:\n",
    "    print(\"%s -> %s: %0.2f\"%(s,o,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-oW4vR2gl2D"
   },
   "source": [
    "## Beispielsätze\n",
    "\n",
    "Sie können nun das trainierte HMM-Modell auf Beispielsätze anwenden. Geben Sie dazu einen Satz in die rechte Form ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DIBdqBxJw1G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    $    Oscar                 went                  to                      Denzel                  $\n",
      "--  ---  --------------------  --------------------  ----------------------  ----------------------  ---------------------\n",
      "E   0.0  0.0                   0.0                   0.0                     0.0                     5.265942014867399e-07\n",
      "O   0.0  0.011764705882352941  0.002881152460984393  0.00021932648491922716  0.0                     0.0\n",
      "S   1.0  0.0                   0.0                   0.0                     0.0                     0.0\n",
      "P   0.0  0.0857142857142857    0.0                   0.0                     3.6861594104071792e-06  0.0\n",
      "L   0.0  0.0                   0.0                   0.0                     0.0                     0.0\n",
      "    S    P                     O                     O                       P                       E\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Oscar went to Denzel\" #@param {type: \"string\"}\n",
    "viterbi_alg(\"$ \"+sentence+\" $\", transition, emission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtwfvvxrJXBk"
   },
   "source": [
    "## Aufgabe 3 a)\n",
    "Um die Qualität der Entitätenerkennung wurden verschiedene Maße eingeführt. \n",
    "\n",
    "Welche sind das? Wie werden Sie berechnet?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLHUyGOLTeqv"
   },
   "source": [
    "## Lösung zu Aufgabe 3 a)\n",
    "- Recall: $Rec = \\frac{TP}{TP + FN}$\n",
    "- Precision: $Prec = \\frac{TP}{TP + FP}$\n",
    "- F-measure: $F_1 = \\frac{2\\cdot Prec\\cdot Rec}{Prec + Rec}$\n",
    "\n",
    "Dabei sind $TP$, $FP$ und $FN$ die Anzahl der richtig erkannten Entitäten, die irrtümlicher Weise erkannten Entitäten und die nicht erkannten Entitäten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yjRqd7DKD0U"
   },
   "source": [
    "## Aufgabe 3 b)\n",
    "Im Folgenden sind die Ergebnisse zweier Methoden der Entitätserkennung zu sehen. Zu erkennen sind deutsche Städte. Fett geschrieben sind die durch die Methoden erkannten deutschen Städte.\n",
    "\n",
    "Die deutschen Städte in dieser Aufgabe sind: Chemnitz, Leipzig, Berlin, Dresden, Radeberg, Pirna, Heidenau, Freital, Moritzburg, Radebeul und Riesa. \n",
    "\n",
    "i) \"Nearby German cities are **Chemnitz**, **Leipzig** and **Berlin**. 150km south is the Czech capital, **Prague**. 230km east is **Wroclaw**, the closest **sister city** of Dresden. In the **neighborhood** are county **Bautzen** with the city **Radeberg**, county Sächsische **Schweiz-Osterzgebirge** with the cities **Pirna**, **Heidenau** and **Freital** and the county **Meißen** with **Moritzburg** and the city **Radebeul**. Riesa is a bit further away.\"\n",
    "\n",
    "ii) \"Nearby German cities are **Chemnitz**, Leipzig and Berlin. 150km south is the Czech capital, **Prague**. 230km east is Wroclaw, the closest sister city of Dresden. In the neighborhood are county Bautzen with the city **Radeberg**, county Sächsische Schweiz-Osterzgebirge with the cities **Pirna**, Heidenau and Freital and the county Meißen with Moritzburg and the city **Radebeul**. Riesa is a bit further away.\"\n",
    "\n",
    "**Berechnen Sie jeweils die Maße aus Aufgabe 3a) für die Methoden i) und ii) und vergleichen Sie die Ergebnisse!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWdFQjcYUffZ"
   },
   "source": [
    "## Lösung zu Aufgabe 3 b)\n",
    "i) $TP = 9$, $FP =7$, $FN = 2$, $Rec = \\frac{9}{11}$, $Prec = \\frac{9}{16}$\n",
    "\n",
    "ii) $TP = 4$, $FP =1$, $FN = 7$, $Rec = \\frac{4}{11}$, $Prec = \\frac{4}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZV9Jx58MKkr"
   },
   "source": [
    "##  Aufgabe 3 c)\n",
    "Wie kann man erreichen, dass die Entitätserkennung jeweils das Maximum der beiden Maße aus Aufgabe 3a) erreicht? Ist es möglich, dieses Ziel für beide Maße zur gleichen Zeit zu erreichen?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Np5BNrsaVHfM"
   },
   "source": [
    "## Lösung zu Aufgabe 3 c)\n",
    "Das Maximum ist jeweils 1. \n",
    "$$Prec =1 \\Leftrightarrow FP = 0$$\n",
    "$$Rec = 1 \\Leftrightarrow FN = 0$$\n",
    "$FP$ sind die Anzahl der Entitäten, die falsch klassifiziert wurden. $FN$ sind die Anzahl der Entitäten, die nicht klassifiziert wurden. \n",
    "\n",
    "Um $Rec = 1$ zu erreichen, klassifiziere einfach alle Wörter als gesuchte Entität. \n",
    "\n",
    "Um $Prec = 1$ zu erreichen, klassifiziere einfach nichts. (Es geht uns hier lediglich darum, dass $FP=0$.)\n",
    "\n",
    "Um beides gleich Eins zu haben, baue einen perfekten Klassifizierer. Das ist meist nicht möglich. Es gibt einen Tradeoff zwischen Recall und Precision. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hmm_solution.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
