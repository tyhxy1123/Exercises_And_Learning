{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Athens, B: NY, qAB: 1.4\n",
      "A: Athens, B: Paris, qAB: 0.84\n",
      "A: Athens, B: Dresden, qAB: 1.05\n",
      "A: Athens, B: Berlin, qAB: 1.05\n",
      "A: Berlin, B: Dresden, qAB: 0.875\n",
      "A: Berlin, B: Paris, qAB: 0.7\n",
      "A: Berlin, B: NY, qAB: 0.0\n",
      "A: Dresden, B: Paris, qAB: 0.7\n",
      "A: Dresden, B: NY, qAB: 0.0\n",
      "A: Paris, B: NY, qAB: 1.4\n"
     ]
    }
   ],
   "source": [
    "count = {\n",
    "    \"Paris\": 5,\n",
    "    \"Berlin\": 4,\n",
    "    \"Dresden\": 4,\n",
    "    \"Athens\": 5,\n",
    "    \"NY\": 1\n",
    "}\n",
    "\n",
    "n = 7;\n",
    "def qAB(nAB, A, B):\n",
    "    q = (n * nAB)/(count[A] * count[B]);\n",
    "    print(\"A: {}, B: {}, qAB: {}\".format(A, B, q)) \n",
    "    return\n",
    "qAB(1, \"Athens\", \"NY\")\n",
    "qAB(3, \"Athens\", \"Paris\")\n",
    "qAB(3, \"Athens\", \"Dresden\")\n",
    "qAB(3, \"Athens\", \"Berlin\")\n",
    "qAB(2, \"Berlin\", \"Dresden\")\n",
    "qAB(2, \"Berlin\", \"Paris\")\n",
    "qAB(0, \"Berlin\", \"NY\")\n",
    "qAB(2, \"Dresden\", \"Paris\")\n",
    "qAB(0, \"Dresden\", \"NY\")\n",
    "qAB(1, \"Paris\", \"NY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-71d6c322bf37>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-71d6c322bf37>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (flew | flight | fly) from * to *\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#(flew | flight | fly) from * to *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test of numpy\n",
    "\n",
    "np = numpy\n",
    "np.log2(64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muster\n",
    "\n",
    "my_list = []\n",
    "\n",
    "attr = [[], ['Private', 'self-employed'], ['<=50K', '>50K']]\n",
    "examples = [[50, \"Private\", \"<=50K\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-35e2595d019e>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-35e2595d019e>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Intelligent Systems TUD 2018, Ex.1\n",
    " \n",
    "Decision Tree Learning:\n",
    "Based on american census data you want to predict two classes of income of people:\n",
    ">50K$, <=50K$.\n",
    "\n",
    "We do not use continuous attributes for this first decision tree task.\n",
    "\"\"\"\n",
    "__author__ = 'Benjamin Guthier'\n",
    "\n",
    "from math import log\n",
    "\n",
    "def openfile(path, fname):\n",
    "    \"\"\"opens the file at path+fname and returns a list of examples and attribute values.\n",
    "    examples are returned as a list with one entry per example. Each entry then\n",
    "    is a list of attribute values, one of them being the class label. The returned list attr\n",
    "    contains one entry per attribute. Each entry is a list of possible values or an empty list\n",
    "    for numeric attributes.\n",
    "    \"\"\"\n",
    "    datafile = open(path + fname, \"r\")\n",
    "    examples = []\n",
    "    for line in datafile:\n",
    "        line = line.strip()\n",
    "        line = line.strip('.')\n",
    "        # ignore empty lines. comments are marked with a |\n",
    "        if len(line) == 0 or line[0] == '|':\n",
    "            continue\n",
    "        ex = [x.strip() for x in line.split(\",\")]\n",
    "        examples.append(ex)\n",
    "\n",
    "    attr = []\n",
    "    for i in range(len(examples[0])):\n",
    "        values = list({x[i] for x in examples}) # set of all different attribute values\n",
    "        if values[0].isdigit():  # if the first value is a digit, assume all are numeric\n",
    "            attr.append([])\n",
    "        else:\n",
    "            attr.append(values)\n",
    "        \n",
    "    return examples, attr\n",
    "\n",
    "\n",
    "def calc_entropy(examples, cls_index):\n",
    "    \"\"\"calculates the entropy over all examples. The index of the class label in the example\n",
    "    is given by cls_index. Can also be the index to an attribute.\n",
    "    \"\"\"\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return result\n",
    "\n",
    "\n",
    "def calc_ig(examples, attr_index, cls_index):\n",
    "    \"\"\"Calculates the information gain over all examples for a specific attribute. The\n",
    "    class index must be specified.\n",
    "    \n",
    "    uses calc_entropy\n",
    "    \"\"\"\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def majority(examples, attr_index):\n",
    "    \"\"\"Returns the value of attribute \"attr_index\" that occurs the most often in the examples.\"\"\"\n",
    "    # create a flat list of all attribute values (with duplicates, so we can count)\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    # among all unique attribute values, find the maximum with regards to occurrence in the attr_vals list\n",
    "    return max(set(attr_vals), key=attr_vals.count)\n",
    "\n",
    "\n",
    "def choose_best_attr(examples, attr_avail, cls_index):\n",
    "    \"\"\"Iterates over all available attributes, calculates their information gain and returns the one\n",
    "    that achieves the highest. attr_avail is a list of booleans corresponding to the list of attributes.\n",
    "    it is true if the attribute has not been used in the tree yet (and is not numeric).\n",
    "    \"\"\"\n",
    "    igs = [] # list of information gains for each attribute\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return igs.index(max(igs)) # return index of the attribute with highest IG\n",
    "\n",
    "\n",
    "def dtree_learning(examples, attr_avail, default, cls_index):\n",
    "    \"\"\"Implementation of the decition tree learning algorithm according to the pseudo code\n",
    "    in the lecture. Receives the remaining examples, the remaining attributes (as boolean list),\n",
    "    the default label and the index of the class label in the attribute vector.\n",
    "    Returns the root node of the decision tree. Each tree node is a tuple where the first entry is\n",
    "    the index of the attribute that has been used for the split. It is \"None\" for leaf nodes.\n",
    "    The second entry is a list of subtrees of the same format. The subtrees are ordered in the\n",
    "    same way as the attribute values in \"attr\". For leaf nodes, the second entry is the predicted class.\n",
    "    \n",
    "    uses choose_best_attr, majority, dtree_learning\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return tree\n",
    "\n",
    "\n",
    "def dtree_classify(dtree, x):\n",
    "    \"\"\"Classifies a single example x using the given decision tree. Returns the predicted class label.\n",
    "    \"\"\"\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return dtree_classify(dtree[1][subtree_pos], x) # descend into subtree recursively\n",
    "\n",
    "\n",
    "def dtree_test(dtree, examples, cls_index):\n",
    "    \"\"\"Classify all examples using the given decision tree. Prints the achieved accuracy.\"\"\"\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    print(\"{} out of {} correct ({:.2f}%)\".format(correct, len(examples), correct/len(examples)*100))\n",
    "\n",
    "\n",
    "path = \"data/\"  #directory of your data\n",
    "datafile = \"adult.data.txt\"\n",
    "testfile = \"adult.test.txt\"\n",
    "examples, attr = openfile(path, datafile) # load the training set\n",
    "test, test_attr = openfile(path, testfile) # load the test set\n",
    "cls_index = len(attr)-1 # the last attribute is assumed to be the class label\n",
    "#attr_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "\n",
    "attr_avail = [] # marks which attributes are available for splitting (not numeric and not the class label)\n",
    "for i in range(len(attr)):\n",
    "    # the list attr[i] contains all possible values of attribute i. It is empty for numeric attributes.\n",
    "    attr_avail.append(len(attr[i])>0 and i != cls_index)\n",
    "\n",
    "dtree = dtree_learning(examples, attr_avail, [], cls_index)\n",
    "dtree_test(dtree, examples, cls_index)\n",
    "dtree_test(dtree, test, cls_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
