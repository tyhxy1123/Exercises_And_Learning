{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXXuyKsuO_Hc"
   },
   "source": [
    "Version: 2019.12.12\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76k0MaK2sz7C"
   },
   "source": [
    "# Intelligent Systems - Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4bKLCWk1Fxx"
   },
   "source": [
    "# Aufgabe 1 - Information Gain\n",
    "\n",
    "* Rufen Sie sich auch den Pseudocode für das Anlernen eines Decision Tree Models ins Gedächtnist:\n",
    "![DTL](https://docs.google.com/uc?id=1U6sPhZkVc0VAykERjN0bJ3ImWU-zNtge)\n",
    "\n",
    "## Aufgabe\n",
    "\n",
    "Betrachten Sie ein Datenset aus 400 Beispiele der Klasse C1 und 400 Beispiele der Klasse C2.\n",
    "Nehmen Sie an, ein Decision Tree Model A teilt diese Beispiele in (300,100) für den ersten Blattknoten und (100,300) für den zweiten Blattknoten, wobei (n,m) bedeutet, dass n Beispiele zu C1 gehören und m Beispiele zu C2.\n",
    "Genau so teilt ein Decision Tree Model B die Beispiele in (200,400) und (200,0).\n",
    "\n",
    "**(1)** Berechnen Sie die Misclassification Rate für beide DTMs und zeigen Sie, dass diese gleich sind!*\n",
    "\n",
    "**(2)** Berechnen Sie den Information Gain für beide DTMs und zeigen Sie, dass der Information Gain von DTM $B$ größer ist als von $A$!*\n",
    "\n",
    "**(3)** Vervollständigen Sie den Code zur Berechnung der Misclassification Rate und  des Information Gains!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGKoPxpQsz7I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for A:  1.0\n",
      "Remaining Entropy R for tree A:  0.8112781244591328\n",
      "Information Gain for tree A:  0.18872187554086717\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "# rest entropy and information gain for A\n",
    "ha = -(400/800*log2(400/800)) - (400/800*log2(400/800))\n",
    "ra = (400/800)*(-(300/400)*log2(300/400)-(1/4)*log2(1/4)) + (1/2)*(-(1/4)*log2(1/4)-(3/4)*log2(3/4))\n",
    "iga = ha - ra\n",
    "\n",
    "print(\"H for A: \", ha)\n",
    "print(\"Remaining Entropy R for tree A: \", ra)\n",
    "print(\"Information Gain for tree A: \", iga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewszVKvnsz7S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for B:  1.0\n",
      "Remaining Entropy R for tree B 0.6887218755408672\n",
      "Information Gain for tree B:  0.31127812445913283\n"
     ]
    }
   ],
   "source": [
    "# rest entropy and information gain for B\n",
    "hb = - 400/800*log2(400/800) - 400/800*log2(400/800)\n",
    "rb = 1/4*(-1*log2(1)-0) + 3/4*(-(1/3)*log2(1/3)-(2/3)*log2(2/3))\n",
    "igb = hb - rb\n",
    "print(\"H for B: \", hb)\n",
    "print(\"Remaining Entropy R for tree B\", rb)\n",
    "print(\"Information Gain for tree B: \", igb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaLqm6UM1JIC"
   },
   "source": [
    "**(4)** Gegeben seien zwei binäre Variablen $A$ und $B$, mit denen ein Decision Tree zur Klassifikation trainiert wird. Dieser verwendet die XOR-Funktion. Die Trainingsdaten bestehen aus Beispielen für alle möglichen Kombinationen von Attributen und Klassen. Anschließend werden ungelabelte (Klasse nicht bekannt) Daten mit allen möglichen Werten für $A$ und $B$ mit dem trainierten Decision Tree klassifiziert. Die klaissifizierten Daten werden wiederrum für das Training eines neuen Decision Trees genutzt. Erhalten wir den gleichen Decision Tree?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG76tMZgsz7Z"
   },
   "source": [
    "# Aufgabe 2 - Recursive Split Algorithm\n",
    "\n",
    "![DTL](https://docs.google.com/uc?id=1U6sPhZkVc0VAykERjN0bJ3ImWU-zNtge)\n",
    "\n",
    "Auf der Basis der Daten der amerikanischen Volkszählung wollen wir die folgenden zwei Klassen bestimmen.\n",
    "\n",
    "    >50K$, <=50K$\n",
    "\n",
    " **Die Attribute sind**: \n",
    "\n",
    "* **age**: continuous.  \n",
    "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n",
    "* **fnlwgt**: continuous. (= final weight)\n",
    "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "* **education-num**:  continuous. \n",
    "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n",
    "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n",
    "* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. \n",
    "(Comment: this term is (was?) quite common in the american litrature, although ...  search the net for the debate in the U.S. and elsewhere.) \n",
    "* **sex**: Female, Male.  \n",
    "* **capital-gain**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **hours-per-week**: continuous.\n",
    "* **native-country**:  United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "Wir benutzen keine kontinuierlichen Attribute. \n",
    "\n",
    "Daten finden Sie auf der Website (adult.txt ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlyq1-Vtsz7c"
   },
   "source": [
    "## Aufgabe: Vervollständigen Sie den unteren Code\n",
    " **Es gibt Blanks in den folgenden Funktionen**: \n",
    "- calc_entropy()\n",
    "- calc_ig()\n",
    "- majority()\n",
    "- choose_best_attr()\n",
    "- dtree_learning()\n",
    "- dtree_classify()\n",
    "- dtree_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm1GxQa0sz7f"
   },
   "outputs": [],
   "source": [
    "\"\"\" Intelligent Systems TUD 2018, Ex.1\n",
    "\n",
    "Decision Tree Learning:\n",
    "Based on american census data you want to predict two classes of income of people:\n",
    ">50K$, <=50K$.\n",
    "\n",
    "We do not use continuous attributes for this first decision tree task.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "__author__ = 'Benjamin Guthier'\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "def openfile(path, fname):\n",
    "    \"\"\"opens the file at path+fname and returns a list of examples and attribute values.\n",
    "    examples are returned as a list with one entry per example. Each entry then\n",
    "    is a list of attribute values, one of them being the class label. The returned list attr\n",
    "    contains one entry per attribute. Each entry is a list of possible values or an empty list\n",
    "    for numeric attributes.\n",
    "    \"\"\"\n",
    "    datafile = open(path + fname, \"r\")\n",
    "    examples = []\n",
    "    for line in datafile:\n",
    "        line = line.strip()\n",
    "        line = line.strip('.')\n",
    "        # ignore empty lines. comments are marked with a |\n",
    "        if len(line) == 0 or line[0] == '|':\n",
    "            continue\n",
    "        ex = [x.strip() for x in line.split(\",\")]\n",
    "        examples.append(ex)\n",
    "\n",
    "    attr = []\n",
    "    for i in range(len(examples[0])):\n",
    "        values = list({x[i] for x in examples})  # set of all different attribute values\n",
    "        if values[0].isdigit():  # if the first value is a digit, assume all are numeric\n",
    "            attr.append([])\n",
    "        else:\n",
    "            attr.append(values)\n",
    "\n",
    "    return examples, attr\n",
    "\n",
    "\n",
    "def calc_entropy(examples, cls_index):\n",
    "    \"\"\"calculates the entropy over all examples. The index of the class label in the example\n",
    "    is given by cls_index. Can also be the index to an attribute.\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    # get attributes of examples with index cls_index\n",
    "    example_classifications = [example[___] for example in examples]\n",
    "    # get unique counts of example classifications\n",
    "    unique, counts = np.unique(___, return_counts=True)\n",
    "\n",
    "    # normalize counts to total number of examples for getting probs\n",
    "    probs = counts/len(examples)\n",
    "    # print(probs)\n",
    "\n",
    "    entropy = ___\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calc_ig(examples, attr_index, cls_index):\n",
    "    \"\"\"Calculates the information gain over all examples for a specific attribute. The\n",
    "    class index must be specified.\n",
    "\n",
    "    uses calc_entropy\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    # get attributes of examples with index attr_index\n",
    "    example_attributes = [example[___] for example in examples]\n",
    "\n",
    "    # get unique counts of example_attributes\n",
    "    unique, counts = np.unique(example_attributes, return_counts=True)\n",
    "\n",
    "    # example split\n",
    "    remainder = 0\n",
    "    for j in range(len(unique)):\n",
    "        # get all examples with attribute unique[j]\n",
    "        examples_unique = [example for example in examples if example[attr_index] == unique[j]]\n",
    "        remainder += len(___)/len(examples)*calc_entropy(___, cls_index)\n",
    "    return calc_entropy(examples, cls_index) - remainder\n",
    "\n",
    "\n",
    "def majority(examples, attr_index):\n",
    "    \"\"\"Returns the value of attribute \"attr_index\" that occurs the most often in the examples.\"\"\"\n",
    "    # create a flat list of all attribute values (with duplicates, so we can count)\n",
    "    attr_vals = [example[___] for example in examples]\n",
    "\n",
    "    # among all unique attribute values, find the maximum with regards to occurrence in the attr_vals list\n",
    "    maj_attr_val = ___\n",
    "    return maj_attr_val\n",
    "\n",
    "\n",
    "def choose_best_attr(examples, attr_avail, cls_index):\n",
    "    \"\"\"Iterates over all available attributes, calculates their information gain and returns the one\n",
    "    that achieves the highest. attr_avail is a list of booleans corresponding to the list of attributes.\n",
    "    it is true if the attribute has not been used in the tree yet (and is not numeric).\n",
    "    \"\"\"\n",
    "    igs = []  # list of information gains for each attribute\n",
    "\n",
    "    for j in range(len(attr)):\n",
    "        if attr_avail[j]:\n",
    "            igs.append(___)\n",
    "        else:\n",
    "            igs.append(-1)\n",
    "\n",
    "    return igs.index(max(igs))  # return index of the attribute with highest IG\n",
    "\n",
    "\n",
    "def dtree_learning(examples, attr_avail, default, cls_index):\n",
    "    \"\"\"Implementation of the decition tree learning algorithm according to the pseudo code\n",
    "    in the lecture. Receives the remaining examples, the remaining attributes (as boolean list),\n",
    "    the default label and the index of the class label in the attribute vector.\n",
    "    Returns the root node of the decision tree. Each tree node is a tuple where the first entry is\n",
    "    the index of the attribute that has been used for the split. It is \"None\" for leaf nodes.\n",
    "    The second entry is a list of subtrees of the same format. The subtrees are ordered in the\n",
    "    same way as the attribute values in \"attr\". For leaf nodes, the second entry is the predicted class.\n",
    "\n",
    "    uses choose_best_attr, majority, dtree_learning\n",
    "    \"\"\"\n",
    "    global attr\n",
    "    if len(examples) == 0:\n",
    "        # is leaf\n",
    "        # no examples left -> majority class prediction of parent (default)\n",
    "        return [None, default]\n",
    "    elif len({x[cls_index] for x in examples}) == 1:\n",
    "        # is leaf\n",
    "        # uniform classes\n",
    "        return [___, ___]\n",
    "    elif attr_avail.count(True) == 0:\n",
    "        # is leaf\n",
    "        # no attribute left to split -> majority class prediction\n",
    "        return [___, ___]\n",
    "    else:\n",
    "        best = choose_best_attr(examples, attr_avail, cls_index)\n",
    "        tree = [best, []]\n",
    "        # append subtrees for each attribute value of 'best'\n",
    "        for v in attr[best]:\n",
    "            examples_v = [x for x in examples if x[___] == v]\n",
    "            new_attr_avail = attr_avail.copy()\n",
    "            new_attr_avail[best] = False\n",
    "            subtree = dtree_learning(___, ___, ___, cls_index)\n",
    "            tree[1].append(subtree)\n",
    "        return tree\n",
    "\n",
    "\n",
    "def dtree_classify(dtree, x):\n",
    "    \"\"\"Classifies a single example x using the given decision tree. Returns the predicted class label.\n",
    "    \"\"\"\n",
    "    # attribute index of splitting attribute\n",
    "    attr_split_index = dtree[___]\n",
    "\n",
    "    if attr_split_index is not ___:\n",
    "        # get attribute value for example\n",
    "        example_split_attr = x[attr_split_index]\n",
    "\n",
    "        # subtree position\n",
    "        subtree_pos = attr[attr_split_index].index(example_split_attr)\n",
    "\n",
    "        return dtree_classify(dtree[1][subtree_pos], x)  # descend into subtree recursively\n",
    "    else:\n",
    "      # leaf, return class prediction\n",
    "        return dtree[1]\n",
    "\n",
    "\n",
    "def dtree_test(dtree, examples, cls_index):\n",
    "    \"\"\"Classify all examples using the given decision tree. Prints the achieved accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    for j in range(len(examples)):\n",
    "        # check if predicted class label for example[j] is the correct one\n",
    "        if dtree_classify(dtree, ___) == ___:\n",
    "            correct += 1\n",
    "\n",
    "    print(\"{} out of {} correct ({:.2f}%)\".format(correct, len(examples), correct / len(examples) * 100))\n",
    "\n",
    "\n",
    "path = \"./\"  # directory of your data\n",
    "datafile = \"adult.data.txt\"\n",
    "testfile = \"adult.test.txt\"\n",
    "examples, attr = openfile(path, datafile)  # load the training set\n",
    "test, test_attr = openfile(path, testfile)  # load the test set\n",
    "cls_index = len(attr) - 1  # the last attribute is assumed to be the class label\n",
    "# attr_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "\n",
    "attr_avail = []  # marks which attributes are available for splitting (not numeric and not the class label)\n",
    "for i in range(len(attr)):\n",
    "    # the list attr[i] contains all possible values of attribute i. It is empty for numeric attributes.\n",
    "    attr_avail.append(len(attr[i]) > 0 and i != cls_index)\n",
    "\n",
    "# print(attr_avail)\n",
    "dtree = dtree_learning(examples, attr_avail, [], cls_index)\n",
    "\n",
    "print(\"Before removal of unknowns: \")\n",
    "print(\"Training Set\")\n",
    "dtree_test(dtree, examples, cls_index)\n",
    "print(\"Test Set\")\n",
    "dtree_test(dtree, test, cls_index)\n",
    "\n",
    "# Extra task removal of unknowns\n",
    "examples_removed = []\n",
    "test_removed = []\n",
    "\n",
    "for example in examples:\n",
    "    # print(example.__contains__(\"?\"))\n",
    "    if not example.__contains__(\"?\"):\n",
    "        examples_removed.append(example)\n",
    "\n",
    "for example in test:\n",
    "    if not example.__contains__(\"?\"):\n",
    "        test_removed.append(example)\n",
    "\n",
    "# print(len(examples_removed))\n",
    "# print(len(test_removed))\n",
    "dtree = dtree_learning(examples_removed, attr_avail, [], cls_index)\n",
    "\n",
    "print(\"After removal of unknowns: \")\n",
    "print(\"Training Set\")\n",
    "dtree_test(dtree, examples_removed, cls_index)\n",
    "print(\"Test Set\")\n",
    "dtree_test(dtree, test_removed, cls_index)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "08_decision_trees.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
