{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Lg7Ks482Q8c"
   },
   "source": [
    "Version: 2020.01.14\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRPJSPbQLkzC"
   },
   "source": [
    "# Intelligente Systeme - Übung Gradientenabstiegsverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTOgbX0dLuVd"
   },
   "source": [
    "## Aufgabe 1 - Gradientenberechnung\n",
    "\n",
    "a) Berechnen Sie die Gradienten der folgenden beiden Funktionen\n",
    "\n",
    "$$f(x, y) = \\frac{1}{x^2+y^2}$$\n",
    "und \n",
    "$$f(x, y) = x^2y.$$\n",
    "\n",
    "b) Schreiben Sie die Update-Gleichungen der beiden Funktionen für das Gradientenabstiegsverfahren auf. Welche Eigenschaft des Gradienten wird hier benutzt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_q9Oyc6Zl8jO"
   },
   "source": [
    "### Lösung - Aufgabe 1\n",
    "a) Für $f(x, y) = \\frac{1}{x^2+y^2}$ ergibt sich \n",
    "\n",
    "$$\\nabla f(x, y) = \\begin{pmatrix}-\\frac{2x}{(x^2+y^2)^2} \\\\ -\\frac{2y}{(x^2+y^2)^2}\\end{pmatrix}$$.\n",
    "\n",
    "Für $f(x, y) = x^2y$ ergibt sich \n",
    "\n",
    "$$\\nabla f(x,y) = \\begin{pmatrix}2xy \\\\ x^2\\end{pmatrix}$$\n",
    "\n",
    "b) Die Update-Gleichungen für $f(x, y) = \\frac{1}{x^2+y^2}$ lauten\n",
    "$$x \\leftarrow x + \\alpha \\frac{2x}{(x^2+y^2)^2}$$\n",
    "$$x \\leftarrow y + \\alpha \\frac{2y}{(x^2+y^2)^2}$$\n",
    "\n",
    "Die Update-Gleichungen für $f(x, y) = x^2y$ lauten\n",
    "\n",
    "$$x \\leftarrow x - 2\\alpha xy$$\n",
    "$$y \\leftarrow y - \\alpha x^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK4WU-6qORC9"
   },
   "source": [
    "## Aufgabe 2 - Lineare Regression\n",
    "Die folgende Tabelle gibt den Treibstoffverbrauch $c$ in $\\frac{l}{100 \\text{km}}$ bei gegebener Fahrtgeschwindigkeit $s$ in $\\frac{\\text{km}}{\\text{h}}$ wieder: \n",
    "\n",
    "|$s$|$c$|\n",
    "|--|--|\n",
    "|0|\t0|\n",
    "|30\t|3.5|\n",
    "|50|5|\n",
    "|80|6.8|\n",
    "|100|7.4|\n",
    "|130|8|\n",
    "|180|\t12|\n",
    "\n",
    "\n",
    "\n",
    "a) Schreiben Sie die Loss-Funktion $\\mathcal{L}(\\vec{w})$ für $n$ Datenpunkte $(s_i, c_i)$ auf. Benutzen Sie eine lineare Funktion $c(s) = w_1 s + w_0$ als Hypothese.\n",
    "\n",
    "b) Leiten Sie die Update-Gleichungen für $w_1$ und $w_0$ her. \n",
    "\n",
    "c) Vervollständigen Sie entsprechend der Update-Gleichungen den untenstehenden Code. Probieren Sie auch unterschiedliche Startwerte $w_0$ und $w_1$ aus. Was passiert für zu große Lernraten $\\alpha$, was für zu kleine Lernraten $\\alpha$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiWK0etTLhPj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def update(w1, w0, alpha, s, c):\n",
    "  n = len(s)\n",
    "  dw1 = 1/n*np.sum((w0 + w1*s - c)*s)\n",
    "  dw0 = 1/n*np.sum(w0 + w1*s - c)\n",
    "\n",
    "  w1 = w1 - alpha*dw1\n",
    "  w0 = w0 - alpha*dw0\n",
    "\n",
    "  return w1, w0\n",
    "\n",
    "\n",
    "s = np.array([0, 30, 50, 80, 100, 130, 180])\n",
    "c = np.array([0, 3.5, 5.0, 6.8, 7.4, 8.0, 12.0])\n",
    "\n",
    "iterations = 100\n",
    "\n",
    "# Startwerte\n",
    "w1 = 2\n",
    "w0 = 2\n",
    "\n",
    "# Lernrate\n",
    "alpha = 0.0001\n",
    "\n",
    "for i in range(iterations):\n",
    "  w1, w0 = update(w1, w0, alpha, s, c)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(r\"$s/\\frac{km}{h}$\")\n",
    "plt.ylabel(r\"$c/\\frac{l}{100km}$\")\n",
    "plt.plot(s, c, '.')\n",
    "plt.plot(s, s*w1 + w0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7fSyBKPTWBx"
   },
   "source": [
    "d) Bestimmen Sie durch Nullsetzen des Gradienten die optimalen $w_0$ und $w_1$ und vergleichen Sie mit der numerisch ermittelten Lösung.\n",
    "\n",
    "**e*)** Auch für die folgende allgemeine Hypothese \n",
    "\n",
    "$$y(x) = \\sum_{i=1}^m w_i f_i(x)$$\n",
    "\n",
    "kann man die Lossfunktion aufschreiben und durch Nullsetzen des Gradienten die optimalen Gewichte $w_i$ bestimmen. Versuchen Sie dies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7a8eKgUynoH7"
   },
   "source": [
    "### Lösung - Aufgabe 2\n",
    "a) Die Loss-Funktion ist gegeben durch \n",
    "\n",
    "$$\\mathcal{L}(\\vec{w}) = \\frac{1}{2n}\\sum_{i=1}^n (c_i - w_1 s_i - w_0)^2$$\n",
    "\n",
    "b) Die Ableitungen von $\\mathcal{L}$ ergeben sich nach \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_0} = - \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0)$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_1} = - \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0) s_i$$\n",
    "\n",
    "Die Update-Gleichungen sind dann entsprechend\n",
    "\n",
    "$$w_0 \\leftarrow w_0 - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_0} = w_0 + \\alpha \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0)$$\n",
    "\n",
    "$$w_1 \\leftarrow w_1 - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_1} = w_1 + \\alpha \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0)s_i$$\n",
    "\n",
    "d) Wir setzen $<x> = \\frac{1}{n}\\sum_{i=1}^n x_i$. Nullsetzen und nutzen dieser Abkürzung ergibt dann \n",
    "\n",
    "$$0 = \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0) = <c> - w_1 <s> - w_0$$\n",
    "\n",
    "$$0 = \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0) s_i = <cs> - w_1 <s^2> - w_0 <s>$$\n",
    "\n",
    "Auflösen nach $w_0$ und $w_1$ ergibt\n",
    "\n",
    "$$w_1 = \\frac{<cs> - <c><s>}{<s^2> - <s>^2}$$\n",
    "\n",
    "$$w_0 = <c> - w_1 <s>$$\n",
    "\n",
    "e) Die Loss-Funktion ist\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\sum_{j=0}^m w_j f_j(x_i)\\right)^2$$.\n",
    "\n",
    "Ableiten nach beliebigem $w_k$ ergibt \n",
    "\n",
    "$$\\frac{\\mathcal{L}}{\\partial w_k} = -\\frac{1}{n} \\sum_{i=1}^n\\left(y_i - \\sum_{j=0}^m w_j f_j(x_i)\\right)f_k(x_i) = - <yf_k(x)> + \\sum_{j=0}^m w_j <f_j(x)f_k(x)> $$\n",
    "\n",
    "bzw. das Gleichungssystem\n",
    "\n",
    "$$b_k = \\sum_{j=0}^m w_ja_{jk}$$\n",
    "mit $b_k = <yf_k(x)>$ und $a_{jk} = <f_j(x)f_k(x)> = a_{kj}$. Dieses kann mit Standardlösungsverfahren gelöst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyslSSr6WONw"
   },
   "source": [
    "## Aufgabe 3 - Visualisierung Gradientenabstiegsverfahren\n",
    "\n",
    "Für die folgende Aufgabe verwenden wir das Doppelmuldenpotential \n",
    "\n",
    "$$V(x) = ax^4 + bx^2 + cx + d$$\n",
    "\n",
    "mit $a = 1$, $b = -3$, $c =1$ und $d = 3.514$. \n",
    "\n",
    "Wir wollen mithilfe des Gradientenabstiegsverfahren das globale Minimum $x_{min}$ dieser Funktion ermitteln. Sie können sich vorstellen, dass $V$ eine Loss-Funktion mit nur einem Gewicht $x$ beschreibt. \n",
    "\n",
    "a) Berechnen Sie die Ableitung und Update-Gleichung für das Gewicht $x$ mit Lernrate $\\alpha$.\n",
    "\n",
    "b) Vervollständigen Sie entsprechend unten stehenden Code.\n",
    "\n",
    "c) Testen Sie die folgenden Kombinationen für Startwert und Lernrate $(x_0, \\alpha)$. \n",
    "\n",
    "$$(x_0, \\alpha) = (-1.75, 0.001)$$\n",
    "$$(x_0, \\alpha) = (-1.75, 0.19)$$\n",
    "$$(x_0, \\alpha) = (-1.75, 0.1)$$\n",
    "$$(x_0, \\alpha) = (-1.75, 0.205)$$\n",
    "\n",
    "d) Wie kann man einen Kompromiss zwischen $(x_0, \\alpha) = (-1.75, 0.001)$ und $(x_0, \\alpha) = (-1.75, 0.19)$ schaffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c93Sqk6DGgLK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def update2(x, a, b, c, d, alpha):\n",
    "  x = x - alpha*(4*a*x**3 + 2*b*x + c)\n",
    "\n",
    "  return x\n",
    "\n",
    "def V(x, a, b, c, d):\n",
    "  return a*x**4 + b*x**2 + c*x + d\n",
    "\n",
    "# TODO: Change to right parameters.\n",
    "a = 1\n",
    "b = -3\n",
    "c = 1\n",
    "d = 3.514\n",
    "\n",
    "x0 = -1.75\n",
    "iterations = 101\n",
    "alphas = np.array([0.001, 0.19, 0.1, 0.205])\n",
    "\n",
    "losses = np.empty(shape=(iterations, len(alphas)))\n",
    "results = np.empty(len(alphas))\n",
    "\n",
    "for j in range(len(alphas)):\n",
    "  x = x0\n",
    "  alpha = alphas[j]\n",
    "  for i in range(iterations):\n",
    "    losses[i, j] = V(x, a, b, c, d)\n",
    "    if i != iterations - 1:\n",
    "      x = update2(x, a, b, c, d, alpha)\n",
    "  results[j] = x\n",
    "\n",
    "for j in range(len(alphas)):\n",
    "  print(100*\"-\")\n",
    "  print(\"Alpha: \", alphas[j])\n",
    "  print(\"xmin: \", results[j])\n",
    "  print(\"Loss: \", V(results[j], a, b, c, d))\n",
    "\n",
    "colors = {\n",
    "    0.001: \"blue\",\n",
    "    0.19: \"red\",\n",
    "    0.1: \"black\",\n",
    "    0.205: \"orange\"\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Lernkurven\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Loss V\")\n",
    "plt.xlim(0, iterations)\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "  alpha = alphas[i]\n",
    "  plt.plot(range(iterations), losses[:, i], label=str(alpha), color=colors[alpha])\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(bottom=0)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Funktion V und Minima\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"V(x)\")\n",
    "\n",
    "xs = np.linspace(-2, 2, 100)\n",
    "ys = V(xs, a, b, c, d)\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "for j in range(len(alphas)):\n",
    "  alpha = alphas[j]\n",
    "  xmin = results[j]\n",
    "  vxmin = V(xmin, a, b, c, d)\n",
    "  plt.plot(xmin, vxmin, marker='.', linestyle=\"None\", label=str(alpha), color=colors[alpha], ms=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaIbuB2Bsmh0"
   },
   "source": [
    "### Lösung - Aufgabe 3\n",
    "\n",
    "a) Die Ableitung ist \n",
    "\n",
    "$$\\partial_x V(x) = 4ax^3 + 2bx + c$$.\n",
    "\n",
    "Die Update-Gleichung ist entsprechend\n",
    "\n",
    "$$x \\leftarrow x - \\alpha \\left(4ax^3 + 2bx + c\\right)$$\n",
    "\n",
    "c) \n",
    "\n",
    "$(x_0, \\alpha) = (-1.75, 0.001)$: Linkes (globales) Minimum wird extrem langsam gefunden ($\\alpha$ zu klein).\n",
    "\n",
    "$(x_0, \\alpha) = (-1.75, 0.19)$: Kein Minimum wird gefunden. Parameter $x$ springt hin und her in der linken Mulde ($\\alpha$ zu groß).\n",
    "\n",
    "$(x_0, \\alpha) = (-1.75, 0.1)$: Linkes Minimum wird gefunden.\n",
    "\n",
    "$(x_0, \\alpha) = (-1.75, 0.205)$: Linkes Minimum wird überschossen. Rechtes lokales Minimum wird gefunden.\n",
    "\n",
    "d) Passe $\\alpha$ an. Starte mit großem $\\alpha$ und reduziere z.B. alle $n$ Epochen um einen Faktor $f$.\n",
    "\n",
    "$\\alpha \\leftarrow f\\cdot\\alpha$ alle $n$ Epochen."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "12_gradient_descent_solution.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
